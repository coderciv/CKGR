In the semantic alignment phase, BERT-large is pre-trained with a batch size of 128 for three epochs. The optimizer is Adam, with learning rates set to 1e-4 for BERT and 5e-5 for MLP. The graph representation learning and decoding phases are implemented in PyTorch and DGL, with an embedding dimension of 500, a convolution kernel size of 3Ã—3, a batch size of 1024, and an initial learning rate of 0.0003. The mean reciprocal rank (MRR) is evaluated on the validation set every 15 epochs during training, and early stopping is applied. Experiments are conducted on a server equipped with four NVIDIA RTX 3090 GPUs.